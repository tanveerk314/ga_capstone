{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing_electra.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f59whCy-vFe6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSFqbUL89geA"
      },
      "source": [
        "Referenced: https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHleGMv-pUGI",
        "outputId": "4989c01b-c057-4fae-f2a5-67ef5997eb02"
      },
      "source": [
        "# uncomment before running on google colab \n",
        "!pip install transformers==4.8.1\n",
        "!pip install datasets "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.8.1\n",
            "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (3.13)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (4.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 43.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 75.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (4.62.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers==4.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.8.1) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.8.1) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.1) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.1) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.1) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.1) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.1) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 49.0 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.2)\n",
            "Collecting huggingface-hub<0.1.0,>=0.0.14\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 39.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 60.7 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 77.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, xxhash, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.0.12\n",
            "    Uninstalling huggingface-hub-0.0.12:\n",
            "      Successfully uninstalled huggingface-hub-0.0.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.8.1 requires huggingface-hub==0.0.12, but you have huggingface-hub 0.0.17 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.12.1 fsspec-2021.8.1 huggingface-hub-0.0.17 multidict-5.1.0 xxhash-2.0.2 yarl-1.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx9rgcE9pfb6"
      },
      "source": [
        "from datasets import load_dataset, load_metric "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296,
          "referenced_widgets": [
            "248c7bb0c446470481f82dd1a5418ece",
            "fa9b4ea00d074c819744129c5283d2ab",
            "389ec72c92414c16baa6d836062e9c9f",
            "5ced3395c2724733a0ce813f06757d8a",
            "230639ad9be54a9da63a851e9407e23a",
            "e11042a78c7a45c5a42d1f648114e450",
            "ed7edbf72d604507b728ea7ec65c1af3",
            "8ca4de37877048a3a141eea62066b086",
            "886e76198bc54ecab7ace6110a74a06a",
            "ca26738b413240c0895a95cead6cf1c8",
            "a4f71d9a9fb24e248d2c7f901823c0d1",
            "76e96b1d409a45eebab0f2fc76648594",
            "585a5eebd9e344deb196b6d73d6cc94c",
            "b8c7dba2a33349cf9cdca1dd1d70a9ac",
            "7359c981028b41609d806c1f62ae03ad",
            "40a40bbba7a043ea81aa36b7246f7c4b",
            "fc2d860ba6f54786b541928027796972",
            "e2bb291da42d44789b89e2ce65b5ddf6",
            "9ea503544c7348c9b704abb00f5ab0fc",
            "ef73b527c08d4cc3a9306d804b91425e",
            "e21b65f9585c46e18304c19be748966f",
            "b65f93017a4f4910880cc9829f1fc227",
            "b64fc1e5f43c42b9afc8da093c4ef059",
            "76dd07875307433da71d2f5bf8d28e90",
            "bd82818a5767413a9a3b6fadba2c17fd",
            "f7a6e15263d54799895782dedbea1c9c",
            "4acdc5ec7f66461586581966da4af585",
            "8047d8a4f3c54d59a0913674671b20a9",
            "e2e7a8c1ac1943bc8b8a4a18d41b46b4",
            "008d9c84bbda4458aeadeb936ed149ad",
            "1fd0b414a5824a318c99616b507724e5",
            "6c30d2d88df04928a7f650a27e77ac54",
            "3d0c8f03947a46f7a33fae40d5c877d2",
            "771051cc63204ae99a04a73e613eacb6",
            "fc91c11c32b8453f9b805d8a1e062727",
            "63e49eecbe744d199bb49835c23f5191",
            "7a3b89b69f7b48e59723f37cda90b68e",
            "6259829e2a4a4040bcf38b7bd4ae28e5",
            "eef7c317d15e404d8e393e46b74a4f7c",
            "900497f9f4614cffb4305e48c932f31b",
            "6b0e6a7380174805a0f2b8c36cf4e820",
            "c1c96569ffc54746a180b2183e6a7a0f",
            "9f72aa116e9a4a6e837930004c1295c6",
            "2c5df1f899e9443183ba4f12059f3561",
            "4a73271866ca46bcb452c0877778e94b",
            "100233993f3f43cf8f7f577807c04e63",
            "5d7cfc85f4f9475992cdb0fec28de140",
            "89fd6352a146411a94c0d20727f20ba6",
            "43931f0dfa1f4532827bd1c245c96422",
            "8d17a3f42ccb41c69976b635dc63ce14",
            "4b689a5e082c4540aa28212869723393",
            "0b2bb2048fb544719b718da96341c9d2",
            "58ddcbd5543c43c4b050486421e0cc8d",
            "5547eef36a8a41a79fe897b169201d73",
            "24b28e867a15443baa688559e8d147bc",
            "f400d656298749f7bfc217868ba7851c",
            "df159c9d75af4c08ba5c88433d12874f",
            "c38f5b845a694f8a8d1f64f4cc6bd124",
            "7b51e77ac8d64ffa9a7cb2da144d52b8",
            "96418d909c524fe4b5a971687f8e0082",
            "697413bb284b47bfa11943623d4a3627",
            "503aa96718464770a980727dd5a716d7",
            "61dbe115add843ad8aa3a555f6374c3b",
            "2d16c98972654a5fba71e9821be1dc58",
            "9e7a225615454d3ead1bd0245c9249cd",
            "a46e70e3e4054c4e9c370fc266e203d2",
            "3b48fa33be0c4d5e9b96615088c37c58",
            "a1b633da540e4233bbfd5763c3a98b0f",
            "9ddf07fd2954418a9452fd798bae762f",
            "ef0ce0a4f0ca4b82868b73e71965197a",
            "b9da018c77ef4dcab92f2112e4e8ef7c",
            "7ea8ae1801e84fec8278503357395c3d",
            "c79a2bf723ef4eaa8387e2b9007b3805",
            "774d982c32c24e4b9cd59a06e9f540ae",
            "2052b665add548fb870a0deba1f614f5",
            "6dab2585c7254c9ab84709f4049ea64c",
            "5690ead76f9b46a788490d96ea9d8361",
            "5fdbe7a410bf48459ee45ab629a1b03d",
            "3be8626a5a874517b705df637e44b758",
            "d2363fe5a7924c438eee2e2131b35d00",
            "e50b090223444328a40ae04b5e95661c",
            "a9d9a18b8885498fb3c3e6bdb57f36d3",
            "195bfaca64ca48789ed57ff10b4f755a",
            "aea929630c4f47de8bff74382f8602b7",
            "6c3cd248d3034787ab85fe5fcd8e39a4",
            "476890fc77f545dbb6145e3a22730d5a",
            "b929f96fafc44c899cb8e8c21b1c05c1",
            "6591f546283342bfac37537745253cc8",
            "c4e2be7ab6984408a1893a5f3753c673",
            "0befcfb8ba704a0cb626e92da1b25d15",
            "08aa9f1e436c4d308ea8b4bafcb6d6ba",
            "8662fbe34f0b40228b6d5c194c5a7d97",
            "2e447d57f760485b9e681522900d8cb4",
            "34e3c89ff41b40ff86419843e4ef0e8f",
            "3c9e105aafd94586b40773706dce3ccd",
            "eb3c3a312b5f408db82d6ec69a0f4231",
            "b57919272a7f41e4bc17435221ed1fd2",
            "f5ecdcecc7d74d15af82820f4f93214b",
            "8ef48678ccad4f8bbdac4e59efff33d9"
          ]
        },
        "id": "2JfenKsxpj6A",
        "outputId": "801818d3-8590-4cfa-83fc-9881386717a2"
      },
      "source": [
        "# load dataset \n",
        "datasets = load_dataset(\"squad_v2\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "248c7bb0c446470481f82dd1a5418ece",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.87k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76e96b1d409a45eebab0f2fc76648594",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b64fc1e5f43c42b9afc8da093c4ef059",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "771051cc63204ae99a04a73e613eacb6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/9.55M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a73271866ca46bcb452c0877778e94b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/801k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f400d656298749f7bfc217868ba7851c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b48fa33be0c4d5e9b96615088c37c58",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fdbe7a410bf48459ee45ab629a1b03d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4e2be7ab6984408a1893a5f3753c673",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdY8T62ypoxT",
        "outputId": "1c88b6f9-4711-4a6d-873a-2e13c45aa18c"
      },
      "source": [
        "# check contents of dataset \n",
        "datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 130319\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 11873\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI1uWdnGptT0",
        "outputId": "9940181d-3bc7-45d0-b782-5e865612a36a"
      },
      "source": [
        "# look at first entry in train set \n",
        "datasets['train'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answers': {'answer_start': [269], 'text': ['in the late 1990s']},\n",
              " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
              " 'id': '56be85543aeaaa14008c9063',\n",
              " 'question': 'When did Beyonce start becoming popular?',\n",
              " 'title': 'Beyoncé'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6aEJGKzpzFh"
      },
      "source": [
        "# Preprocess train data \n",
        "\n",
        "- In order to train a model with our text data we will need to tokenize it and get word embedding vectors. \n",
        "\n",
        "- In Hugging Face each model comes with a tokenizer that does most of the heavy lifting for us. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "2f63d6ae55de401ebf2e04b1d39e478a",
            "559c99d16703439484eb85b5382f79fa",
            "cf5a9e30a0f34cb390025b032d1e9962",
            "1a46551527d843a9826d117f663fa820",
            "cf158edd9db14de7a384f052d399cb67",
            "7e2adfb327474de3955f9c83e2505c26",
            "546fbe2cf43e41aab5fc8e04f2a17884",
            "f83df7fe14e64687be4a7c4d819ad361",
            "42e7ff9a68fd42c2bd0ee85a4c7f87cc",
            "2db070c4a8414d018ab2c3201ca2eee1",
            "3ba344087c4441d0a88b980822afb83c",
            "57f75788addd49dc8fbae1ff0e000693",
            "4b69c2638b6c4e9592029f052d67e51d",
            "2dc86760c3f14c488ca801a5ff1ae2c9",
            "01801c719bf843bd802b4053e108d2da",
            "18d2d5b5d9964efaaae2c6e2c1bb130b",
            "fd56fa15299b491f9f721edfd1aabeda",
            "38564a69870a4e00903fd9c046feab6c",
            "ab814f5eaa6b4a08b15884d834089853",
            "239b5eb860aa4ecc9e8a094120959978",
            "2ef2638f285f46b284691d069435599c",
            "1c797329d9e14d55b84de8678220b27c",
            "a91d4ee028c54a6ab3495d6f57f15cbc",
            "33d94aa4650645989fa179faf007e4e3",
            "321dc057349f4d5da74ffb711f17728d",
            "ac8f71f4da1448f4bcdefdfdc195669e",
            "0a1364092d424ca68fd74a153a68b8f3",
            "97ffc6945c9743488c8c445c2fe98f80",
            "d8b4e7f7cfac41fcb80dbe737107b89c",
            "74ea012003f147c79a5c11da94f0af71",
            "8b6f96205a434e20b4855918a63b6706",
            "66cc08df5bd14643ac0daf365b3a9e18",
            "9d8f41b354124a2eb2ea01e470c71678",
            "0e736ac3172b4470a123c23272eea9c9",
            "7fcfa9da9e36470999b34b84e6ebb82f",
            "22b29ebe524d468ebbe61f01cf741c26",
            "7100a80110b84c05937aaa537299bed3",
            "18b80d02133c48b1bf78ae2a67ccb8b0",
            "3515fcac5fa440b9850be006f02d83ad",
            "c827a0079a974689bfa8260279d75fbf",
            "ef9c601ef4da4a5abbe0c2ef18a716c7",
            "622ec94f6b14465e951a0ccf5e165d2d",
            "07ef3d63b93d4d498e09e48749b6155f",
            "b759336d97e94449abdbedcba4604018"
          ]
        },
        "id": "uEKiBpMup6f-",
        "outputId": "53b4bd39-45fa-4539-fc01-f7611dc89854"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# instantiate tokenizer instance for same model being used \n",
        "tokenizer = AutoTokenizer.from_pretrained('google/electra-small-discriminator')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f63d6ae55de401ebf2e04b1d39e478a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57f75788addd49dc8fbae1ff0e000693",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a91d4ee028c54a6ab3495d6f57f15cbc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e736ac3172b4470a123c23272eea9c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAWpUQvEqBb2",
        "outputId": "21176fd8-0164-4c14-8f23-96695425cf71"
      },
      "source": [
        "type(tokenizer) # confirm fast tokenizer "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.electra.tokenization_electra_fast.ElectraTokenizerFast"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sZjch8N8i_x"
      },
      "source": [
        "\n",
        "\n",
        "\"Fast\" tokenizers in the Hugging Face library provide several additional features that allow us to go back and forth between the original string representation and the token space. This will prove useful when trying to convert predictions from the model back to a string representation for evaluation purposes.   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIvgQS3_MvMf",
        "outputId": "4289fb19-34b4-44b7-b099-6a15dde6a8f9"
      },
      "source": [
        "tokenizer.model_max_length # check max size of tokens accepted by model "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7JL8zvp8_hJ"
      },
      "source": [
        "\n",
        "\n",
        "The model we have selected has a maximum token length of 512. If we have a question-context pair that is longer then 512 tokens it will not be possible to represent that with one feature. In this case, if we were to simply discard any tokens past the max then we could possibly be discarding the answer. \n",
        "\n",
        "We can work around this by allowing examples with longer text input to be split into multiple features of size less than (or equal to) the maximum allowed by the model. \n",
        "\n",
        "In order to deal with the possibility that the answer is at or near the point of split for a longer context, we can allow some overlap between the multiple features for that particular example. The amount of overlap we allow between two features from the same example is often referred to as the stride or doc stride. \n",
        "\n",
        "Fortunately, the tokenizer class in HuggingFace has built in functionality allowing us to work with inputs bigger then the maximum allowed by the model.\n",
        "\n",
        "For our purposes we will use 512 as the max length allowed for a feature. By using the max length posible we minimize the number of features for each example. \n",
        "\n",
        "We will use a doc stride of 128. This is a quarter of the size of the max token length. By using such a large doc stride we are trying to minimze the chance that the answer to a question will get lost when splitting a long piece of text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLtMY-mDBObL"
      },
      "source": [
        "max_length = 512 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGhAvidmGSt0"
      },
      "source": [
        "Lets consider an example from the dataset which has text input that is longer then the max token length supported by the model. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swKPdPmx_plO",
        "outputId": "8a4841c6-00d2-4505-ec03-60fed06c1f71"
      },
      "source": [
        "# get example from dataset that is longer then max_length \n",
        "for i,example in enumerate(datasets['train']):\n",
        "  # check if length of encoding of question + context is greater then max of model  \n",
        "  if len(tokenizer(example[\"question\"], example[\"context\"])['input_ids']) > max_length:\n",
        "    break \n",
        "long_example = datasets['train'][i]\n",
        "long_example\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answers': {'answer_start': [3], 'text': ['1565']},\n",
              " 'context': 'In 1565, the powerful Rinbung princes were overthrown by one of their own ministers, Karma Tseten who styled himself as the Tsangpa, \"the one of Tsang\", and established his base of power at Shigatse. The second successor of this first Tsang king, Karma Phuntsok Namgyal, took control of the whole of Central Tibet (Ü-Tsang), reigning from 1611–1621. Despite this, the leaders of Lhasa still claimed their allegiance to the Phagmodru as well as the Gelug, while the Ü-Tsang king allied with the Karmapa. Tensions rose between the nationalistic Ü-Tsang ruler and the Mongols who safeguarded their Mongol Dalai Lama in Lhasa. The fourth Dalai Lama refused to give an audience to the Ü-Tsang king, which sparked a conflict as the latter began assaulting Gelug monasteries. Chen writes of the speculation over the fourth Dalai Lama\\'s mysterious death and the plot of the Ü-Tsang king to have him murdered for \"cursing\" him with illness, although Chen writes that the murder was most likely the result of a feudal power struggle. In 1618, only two years after Yonten Gyatso died, the Gelug and the Karma Kargyu went to war, the Karma Kargyu supported by the secular Ü-Tsang king. The Ü-Tsang ruler had a large number of Gelugpa lamas killed, occupied their monasteries at Drepung and Sera, and outlawed any attempts to find another Dalai Lama. In 1621, the Ü-Tsang king died and was succeeded by his young son Karma Tenkyong, an event which stymied the war effort as the latter accepted the six-year-old Lozang Gyatso as the new Dalai Lama. Despite the new Dalai Lama\\'s diplomatic efforts to maintain friendly relations with the new Ü-Tsang ruler, Sonam Rapten (1595–1657), the Dalai Lama\\'s chief steward and treasurer at Drepung, made efforts to overthrow the Ü-Tsang king, which led to another conflict. In 1633, the Gelugpas and several thousand Mongol adherents defeated the Ü-Tsang king\\'s troops near Lhasa before a peaceful negotiation was settled. Goldstein writes that in this the \"Mongols were again playing a significant role in Tibetan affairs, this time as the military arm of the Dalai Lama.\"',\n",
              " 'id': '56ce4a58aab44d1400b88668',\n",
              " 'question': 'When were the Rinbung princes overthrown?',\n",
              " 'title': 'Sino-Tibetan_relations_during_the_Ming_dynasty'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v10M2yv0HmxH"
      },
      "source": [
        "Lets tokenize the long_example to get a better idea of how the tokenizer works. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_d46j10H2KE",
        "outputId": "1eff2440-caeb-4564-c434-24819c57800f"
      },
      "source": [
        "tokenized_example = tokenizer(\n",
        "    long_example['question'],\n",
        "    long_example['context'],)\n",
        "print(len(tokenized_example['input_ids']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BkfdYVfIJ7b"
      },
      "source": [
        "If we do not truncate the input the tokenizer will return a token that is larger then what the model will accept. \n",
        "\n",
        "Lets consider truncating and restricting the size of the output tokens to the maximum size allowed by our model since we are looking to feed this data to our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgXlyqXyIler",
        "outputId": "dbeb7423-d5b9-47f2-f4c0-013ee6a6a17d"
      },
      "source": [
        "tokenized_example = tokenizer(\n",
        "    long_example['question'],\n",
        "    long_example['context'],\n",
        "    max_length= max_length,\n",
        "    truncation= \"only_second\",\n",
        "    )\n",
        "print(tokenized_example.keys())\n",
        "print('token_type_ids: ',tokenized_example['token_type_ids'])\n",
        "print(f\"size of feature: {len(tokenized_example['input_ids'])}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
            "token_type_ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "size of feature: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkRitPDQCIi1"
      },
      "source": [
        "The input_ids are the indices corresponding to each token in our sentence. The `input_ids` are the result of mapping the text input to the word embeddings of the specific transformer model. \n",
        "\n",
        "We gave the tokenizer two seperate string inputs (question and context) and it returned a single vector in `input_id` to represent the question-context pair. \n",
        "  - The `token_type_ids` gives a binary mask that tells us whether a particular input_id is part of the first text input (question) or seond (context). The 0 indicates the token is part of the question and 1 indicates it is part of the context. \n",
        "\n",
        "https://huggingface.co/transformers/preprocessing.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w2U6jisqKll"
      },
      "source": [
        "tokenized_example = tokenizer(\n",
        "    long_example['question'],\n",
        "    long_example['context'],\n",
        "    max_length= max_length,\n",
        "    truncation= \"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    stride=doc_stride,\n",
        "    padding=True\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTPOLC3B5p-g"
      },
      "source": [
        "For our running example, adding the following arguments to our tokenizer allows us to be able work with text inputs that are larger then the maximum allowed by the model. \n",
        "  - The `padding=True` arguments ensures that every feature is of the same length for modeling purposes. It does this by adding a placeholder value for indicies after the end of the text until the max length allowed by the model. \n",
        "  - As mentioned earlier the `stride` argument defines the overlap between different features from the sample example. This allows the model to extract meaning from portions of the text that are close to the cut off point. \n",
        "  - The `return_overflowing_tokens` argument returns a mapping that allows us to go from the current span or feature back to the original text input. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd-svVDAEktU",
        "outputId": "35f9e8b7-b824-47b8-ec1e-cd332f3ef521"
      },
      "source": [
        "tokenized_example.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evvh3lfTFUAN",
        "outputId": "a15c600e-c134-446c-cf0e-b565c9e03489"
      },
      "source": [
        "print('number of features/spans: ',len(tokenized_example['input_ids'])) # number of features for example \n",
        "print('length of first feature: ',len(tokenized_example['input_ids'][0])) # size of first feature \n",
        "print('length of second feature: ', len(tokenized_example['input_ids'][1])) # size of second feature \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of features/spans:  2\n",
            "length of first feature:  512\n",
            "length of second feature:  512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r6MMM-RBM4D"
      },
      "source": [
        "After adding the additional arguments to pad and return over flowing tokens we see that \n",
        "  - For the long example (bigger then max allowed by model) we now have two features to represent the text. This is evident from the fact that we have two `input_ids` from the single input. \n",
        "  - Each feature is of the same size as a result of padding \n",
        "\n",
        "- Without padding and truncating the text had 518 tokens. After padding and truncating the second span has 6 new tokens (and the 128 for the stride) representing the rest of the text while the remainder of the tokens are simply placeholders. \n",
        "  - If we could indicate to our model which tokens were actual information from text and which are simply placeholders then it could focus only on the actual input saving computing resources and avoid possibly predicting that the answer is in the padding. \n",
        "  - The attention mask provides exactly this information!  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haJ_vA0BBASE",
        "outputId": "1aac9bae-7f9c-41a6-8d4c-8b01d863af7e"
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "np.array(tokenized_example['attention_mask'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YMyMunmSWon"
      },
      "source": [
        "- We can see that the attention mask for the first span is all 1's indicating that it is all part of the text input. \n",
        "- The attention mask for the second span is 1 if the tokens that represent actual text and 0 if the value in that place is just for padding purposes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuvBJJwmS18S",
        "outputId": "c87dc911-4df5-443e-e99f-6fde0f432e8c"
      },
      "source": [
        "tokenized_example.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV-kJsdMUuwh"
      },
      "source": [
        "  When the input is too long, it's converted in a batch of inputs with overflowing tokens\n",
        "            # and a stride of overlap between the inputs. If a batch of inputs is given, a special output\n",
        "            # \"overflow_to_sample_mapping\" indicate which member of the encoded batch belong to which original batch sample.\n",
        "\n",
        "For each span in the encoded batch the `overflow_to_sample_mapping` tells us which original batch sample that span corresponds to. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xZNEfHnS8Ik",
        "outputId": "72037b51-39b6-4353-ccdf-02b67343f501"
      },
      "source": [
        "tokenized_example['overflow_to_sample_mapping']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFjVOyz_XQJQ"
      },
      "source": [
        "In this particular case, since we only tokenized one example which resulted in two spans the `overflow_to_sample_mapping` gives a value of 0 for each span. This indicates that both tokenized spans originated from the same input.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnODoRxQHZWE"
      },
      "source": [
        "Now this will give us some work to properly treat the answers: we need to find in which of those features the answer actually is, and where exactly in that feature.\n",
        "\n",
        "Thankfully, the tokenizer we're using can help us with that by returning an offset_mapping: \n",
        "\n",
        "Now that we have split up the original question-context pair into multiple spans, we need to find which of the spans contains the answer and where in that span it is located. We can use the `return_offset_mapping` argumentto help us with this. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wDUCfbvIcnj",
        "outputId": "c5189286-3a4d-42d7-8fbb-5bd4ee5af019"
      },
      "source": [
        "# tokenize with offset mappings \n",
        "tokenized_example = tokenizer(\n",
        "    long_example[\"question\"],\n",
        "    long_example[\"context\"],\n",
        "    max_length=max_length,\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    stride=doc_stride,\n",
        "    return_offsets_mapping=True,\n",
        ")\n",
        "print(tokenized_example[\"offset_mapping\"][0][:10])\n",
        "print(tokenized_example['input_ids'][0][:10])\n",
        "print(len(tokenized_example['input_ids'][0])==len(tokenized_example[\"offset_mapping\"][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0), (0, 4), (5, 9), (10, 13), (14, 16), (16, 18), (18, 21), (22, 29), (30, 39), (39, 40)]\n",
            "[101, 2043, 2020, 1996, 15544, 27698, 5575, 12000, 16857, 2078]\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TML-pZXMYwd6",
        "outputId": "897a7fcf-01ae-45d1-e1ca-a4dbe2ab52be"
      },
      "source": [
        "# testing offset mapping \n",
        "print(long_example[\"question\"][:4])\n",
        "print(long_example[\"question\"][5:9])\n",
        "print(long_example[\"question\"])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When\n",
            "were\n",
            "When were the Rinbung princes overthrown?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA_T-bswI9Jx"
      },
      "source": [
        "For each token id in our `input_ids` the offset mapping gives us the corresponding start and end character index in the original text. \n",
        "\n",
        "The very first token ([CLS]) has (0, 0) because it doesn't correspond to any part of the question/answer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Oq0iCYBqw9Z",
        "outputId": "dac61ff2-2768-4d8d-9352-56840865acfb"
      },
      "source": [
        "# show how to go from offset mapping back to original tokens \n",
        "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
        "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
        "print(tokenizer.convert_ids_to_tokens([first_token_id]), example[\"question\"][offsets[0]:offsets[1]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['when'] When\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQNYWSMKJdKn"
      },
      "source": [
        "The offset mapping gives us a way to convert between token index and character index in original text. This mapping can be used to get the position of start/end tokens of our answer in a particular feature. \n",
        "\n",
        "EXPLAIN \n",
        "We just have to distinguish which parts of the offsets correspond to the question and which part correspond to the context\n",
        "\n",
        "this is where the sequence_ids method of our tokenized_example can be useful:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98zhmOHaq4oX",
        "outputId": "6163b337-86ef-4744-e509-81fb9497197b"
      },
      "source": [
        "sequence_ids = tokenized_example.sequence_ids()\n",
        "sequence_ids[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OClquzreJrCM"
      },
      "source": [
        "The sequence_ids method tells us which part of the text input each token was: \n",
        "  - returns None for special tokens\n",
        "  - returns 0 for tokens from the first sequence\n",
        "  - returns 1 for tokens from the second sequence. \n",
        "\n",
        "In our case, we are inputing the question as the first sequence and the context as the second sequence in the tokenizer. Therefore tokens with sequence_ids value of 0 correspond to tokens from question and 1 corresponding to tokens from the context. \n",
        "\n",
        "When looking for answers we should make sure they are not part of the question. This avoids possibly returning the question as the predicted answer.  \n",
        "\n",
        "\n",
        "\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeYSAUIlq6P4",
        "outputId": "88c484d7-7ec5-45d9-aaef-b1b8e00cf226"
      },
      "source": [
        "# get answer to question\n",
        "answers = long_example['answers']\n",
        "# get index where answer starts \n",
        "start_char_index = answers['answer_start'][0]\n",
        "# end index = start index + length of answer \n",
        "end_char_index = start_char_index + len(answers['text'][0])\n",
        "\n",
        "\n",
        "# get start token index of the current span in the text \n",
        "token_start_index = 0 # start at 0 \n",
        "while sequence_ids[token_start_index] != 1: # while token_start index is not in context \n",
        "  token_start_index +=1 # keep incrementing until it hits the first token in the context  \n",
        "\n",
        "# get end token index of the current span in the text \n",
        "token_end_index = len(tokenized_example['input_ids'][0]) -1 # starts at end of tokens  \n",
        "while sequence_ids[token_end_index] != 1:  # look for  value in sequence_ids that is 1 indicating in context \n",
        "# if you dont find it shift left (look at smaller index) \n",
        "  token_end_index -=1  \n",
        "\n",
        "# Detect if the answer is out of the span(in which case this feature is labeled with CLS)\n",
        "offsets = tokenized_example[\"offset_mapping\"][0] \n",
        "# if answer starts and ends within current span  \n",
        "if (offsets[token_start_index][0] <= start_char_index and offsets[token_end_index][1] >= end_char_index): \n",
        "    # Move the token_start_index and token_end_index to the two ends of the answer.\n",
        "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char_index: \n",
        "        token_start_index += 1 # keep shifting to the right until token_start_index points to start index of answer in context\n",
        "    start_position = token_start_index - 1 \n",
        "    while offsets[token_end_index][1] >= end_char_index: \n",
        "        token_end_index -= 1 # keep shifting left until token_end_index points to index of end of answer in context \n",
        "    end_position = token_end_index + 1 \n",
        "    print(start_position, end_position)\n",
        "else:\n",
        "    print(\"The answer is not in this feature.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxkluMwkLoVl"
      },
      "source": [
        "\n",
        "\n",
        "The tokenizer class in hugging face library also allows us to go from input ids back to the original string using the decode method. We can use this to confirm that we got correct start and end positions in terms of the input ids/token index "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBs4edBJq9XF",
        "outputId": "6b036114-80c8-47b0-b948-55e394c54e17"
      },
      "source": [
        "# confirm answer\n",
        "print(tokenizer.decode(tokenized_example['input_ids'][0][start_position:end_position+1]))\n",
        "print(answers['text'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1565\n",
            "1565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqbzgLgaMAqd"
      },
      "source": [
        "\n",
        "Now that we know how to prepare features for modeling we can put all the previous steps into a function for use on our entire dataset. \n",
        "\n",
        "For the case that the answer is not within the current span/feature we can set the start and end position to 0 indicating no answer in this feature. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYkoA7LDrBii"
      },
      "source": [
        "def prepare_train_features(examples):\n",
        "    ''' given examples from Squad dataset: \n",
        "     accounts for examples longer then 512 tokens \n",
        "    returns: tokenized examples with: input ids, attention_mask, answer start position index, answer end position index''' \n",
        "\n",
        "    # tokenize examples accounting for some of them being to long to fit in a single feature \n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # get mapping from features to corresponding example in dataset  \n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    #  get offset_mapping to map tokens to character position in original context  \n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # add keys for start_positions and end_positions of answers to tokenized_examples \n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    # iterate through all offset_mappings (the corresponding start and end character in the original text that gave our token.)\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i] # input ids for tokenized examples \n",
        "        #cls_index = input_ids.index(tokenizer.cls_token_id)         # get cls index to label impossible answers \n",
        "\n",
        "\n",
        "        # Grab the sequence  corresponding for example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans,  \n",
        "        sample_index = sample_mapping[i] # index of the example containing this span of text.\n",
        "        answers = examples[\"answers\"][sample_index] # get answers for this example \n",
        "        # If no answers are given for this example set the answer start and end position to 0 \n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(0)\n",
        "            tokenized_examples[\"end_positions\"].append(0)\n",
        "        else: # answers are given \n",
        "            # get start and end character index of the answer in the context.\n",
        "            start_char_index = answers[\"answer_start\"][0]\n",
        "            end_char_index = start_char_index + len(answers[\"text\"][0])\n",
        "\n",
        "            # get start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            # get end token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # if the answer is out of the span \n",
        "            if not (offsets[token_start_index][0] <= start_char_index and offsets[token_end_index][1] >= end_char_index):\n",
        "              #  set start and end position for answer to 0 since not in this span\n",
        "                tokenized_examples[\"start_positions\"].append(0)\n",
        "                tokenized_examples[\"end_positions\"].append(0)\n",
        "            else: # the answer is in the span \n",
        "            # Move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char_index: \n",
        "                    token_start_index += 1 # keep shifting to the right until token_start_index refers to where the answer starts in text \n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char_index: # start on right side and work down\n",
        "                    token_end_index -= 1 # keep shifting left until token_end_index points to where answer ends in text \n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "d7766ba26be54e17bb3a14975f0e6b4c",
            "533b824e3a6c474aa24dfa6028fc6e80",
            "34e6f26013774d38a4f640796779c8a5",
            "67dea1a4f8ba4437aaa1d2dd9b382864",
            "3b0e29ce9185411f8fb5f93c74fcd5f4",
            "a74311d654ba414cb63480abf571622b",
            "59d4113ff2794a93b76e64a2b632d22c",
            "f7a1610383ad457ba034b8e87467e800",
            "705b8d80d2d047348cfff6c9dee5e148",
            "f6cd167961114e108760efb3632ba441",
            "7ca7ad575e544af5960b9ffb6c93d6ed",
            "c5f1e1592257456e883c6ad930d57a6b",
            "c4fa53044ac64da1917da4941baa2f0f",
            "7902de409f304853a320fb43e75cea13",
            "8237e548bbe1456b948bd7d774d96b80",
            "12251c912e5142ffb58aa108f3f6bdc8",
            "ca21ef1b30884e1aad76c0ed3bf574d2",
            "a893e790843f44708e10d22372fb0469",
            "3eb2b5f954a8477baae6895f48af9702",
            "926376ac51514f29bdfa9b85fe428e01",
            "b0e64797ab4b425e90a483ae549fdcab",
            "f73adead9d624054b0da009955148d46"
          ]
        },
        "id": "7JPJOnNYrOaU",
        "outputId": "eb0bbf66-9057-4744-b629-84b811880361"
      },
      "source": [
        "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7766ba26be54e17bb3a14975f0e6b4c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/131 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5f1e1592257456e883c6ad930d57a6b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/12 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}